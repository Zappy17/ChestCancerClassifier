{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -q dagshub mlflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PUxz2LkuBr7k",
        "outputId": "e6d1b07d-51d1-425f-d7db-1176557eb303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m733.7/733.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_vbjVPlVCZ_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU3iYQQ9bDnv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import timm\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import sklearn\n",
        "import glob\n",
        "import shutil\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji0OFMAqhi-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942a9cbd-ffcc-4cf8-ae10-5c7c61b66b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[/content/data.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/data.zip or\n",
            "        /content/data.zip.zip, and cannot find /content/data.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!unzip -q /content/data.zip -d /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CrDnYrKrOyC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6NdBenSPCKm",
        "outputId": "ee65c6db-e381-4351-879c-74e535362b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/dataset': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls /content/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhJUnPtDNZ45",
        "outputId": "72f94aee-5639-401e-9ab7-4f8eae345a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images found: 2097\n",
            "\n",
            "âœ… Train/Val/Test split completed and saved to: /content/split_data\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# STEP 1: Setup\n",
        "original_dataset_dir = \"/content/dataset\"\n",
        "output_dir = \"/content/split_data\"         # Output split folder\n",
        "\n",
        "classes = [\"Bengin cases\", \"Malignant cases\", \"Normal cases\"]\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# STEP 2: Load image paths and class labels\n",
        "for cls in classes:\n",
        "    cls_folder = os.path.join(original_dataset_dir, cls)\n",
        "    for img_path in glob.glob(os.path.join(cls_folder, '*')):\n",
        "        image_paths.append(img_path)\n",
        "        labels.append(cls)\n",
        "\n",
        "print(f\"Total images found: {len(image_paths)}\")\n",
        "\n",
        "# STEP 3: Split into Train+Val (80%) and Test (20%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# STEP 4: Split Train+Val into Train (80%) and Val (20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42\n",
        ")\n",
        "\n",
        "# Helper function to copy files into target folder\n",
        "def save_split(split_name, X, y):\n",
        "    for path, label in zip(X, y):\n",
        "        split_folder = os.path.join(output_dir, split_name, label)\n",
        "        os.makedirs(split_folder, exist_ok=True)\n",
        "        shutil.copy(path, split_folder)\n",
        "\n",
        "# STEP 5: Copy images to respective folders\n",
        "save_split(\"train\", X_train, y_train)\n",
        "save_split(\"val\", X_val, y_val)\n",
        "save_split(\"test\", X_test, y_test)\n",
        "\n",
        "# âœ… DONE\n",
        "print(\"\\nâœ… Train/Val/Test split completed and saved to:\", output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbVgCuIcxRJX",
        "outputId": "1fa8054b-a1e9-4bfe-f8ae-9f1fbd2c7a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: defaultdict(<class 'int'>, {'Normal cases': 403, 'Malignant cases': 359, 'Bengin cases': 579})\n",
            "Validation set: defaultdict(<class 'int'>, {'Normal cases': 101, 'Malignant cases': 90, 'Bengin cases': 145})\n",
            "Test set: defaultdict(<class 'int'>, {'Normal cases': 127, 'Malignant cases': 112, 'Bengin cases': 181})\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "def count_images_per_class(root_dir):\n",
        "    class_counts = defaultdict(int)\n",
        "\n",
        "    for class_name in os.listdir(root_dir):\n",
        "        class_path = os.path.join(root_dir, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            image_files = [f for f in os.listdir(class_path)\n",
        "                           if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "            class_counts[class_name] = len(image_files)\n",
        "\n",
        "    return class_counts\n",
        "\n",
        "# Example usage:\n",
        "train_dir = \"/content/split_data/train\"\n",
        "val_dir = \"/content/split_data/val\"\n",
        "test_dir = \"/content/split_data/test\"\n",
        "\n",
        "print(\"Train set:\", count_images_per_class(train_dir))\n",
        "print(\"Validation set:\", count_images_per_class(val_dir))\n",
        "print(\"Test set:\", count_images_per_class(test_dir))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from collections import defaultdict\n",
        "\n",
        "# Settings\n",
        "train_dir = \"/content/split_data/train\"\n",
        "target_class_count = 600  # Max class count in training set\n",
        "\n",
        "# Define augmentation generator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Count current images\n",
        "def count_images(root_dir):\n",
        "    class_counts = defaultdict(int)\n",
        "    for class_name in os.listdir(root_dir):\n",
        "        class_folder = os.path.join(root_dir, class_name)\n",
        "        image_files = [f for f in os.listdir(class_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        class_counts[class_name] = len(image_files)\n",
        "    return class_counts\n",
        "\n",
        "class_counts = count_images(train_dir)\n",
        "print(\"Before balancing:\", class_counts)\n",
        "\n",
        "# Apply balancing\n",
        "for class_name, count in class_counts.items():\n",
        "    if count >= target_class_count:\n",
        "        continue  # Already balanced\n",
        "\n",
        "    class_folder = os.path.join(train_dir, class_name)\n",
        "    image_files = [f for f in os.listdir(class_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    needed = target_class_count - count\n",
        "    print(f\"\\nğŸ”„ Augmenting '{class_name}' ({count} â†’ {target_class_count}), need {needed} more\")\n",
        "\n",
        "    i = 0\n",
        "    while i < needed:\n",
        "        for img_name in image_files:\n",
        "            img_path = os.path.join(class_folder, img_name)\n",
        "            img = tf.keras.preprocessing.image.load_img(img_path)\n",
        "            x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "            x = x.reshape((1,) + x.shape)\n",
        "\n",
        "            prefix = os.path.splitext(img_name)[0]\n",
        "\n",
        "            for batch in datagen.flow(x, batch_size=1, save_to_dir=class_folder,\n",
        "                                      save_prefix=f\"{prefix}_aug\", save_format='jpeg'):\n",
        "                i += 1\n",
        "                if i >= needed:\n",
        "                    break\n",
        "            if i >= needed:\n",
        "                break\n",
        "\n",
        "# Re-count\n",
        "balanced_counts = count_images(train_dir)\n",
        "print(\"\\nâœ… After balancing:\", balanced_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbDAAqvj7dHb",
        "outputId": "1bae33c5-1afe-4832-c97d-2f710a3bd97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before balancing: defaultdict(<class 'int'>, {'Normal cases': 506, 'Malignant cases': 359, 'Bengin cases': 579})\n",
            "\n",
            "ğŸ”„ Augmenting 'Normal cases' (506 â†’ 600), need 94 more\n",
            "\n",
            "ğŸ”„ Augmenting 'Malignant cases' (359 â†’ 600), need 241 more\n",
            "\n",
            "ğŸ”„ Augmenting 'Bengin cases' (579 â†’ 600), need 21 more\n",
            "\n",
            "âœ… After balancing: defaultdict(<class 'int'>, {'Normal cases': 598, 'Malignant cases': 597, 'Bengin cases': 600})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLKr_B_VbZf5",
        "outputId": "04b5b5a3-7e6f-43c8-f6f8-e148974594b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Classes: ['Bengin cases', 'Malignant cases', 'Normal cases']\n",
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [01:00<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1312 | Train Acc: 0.4730\n",
            "Val Loss: 0.9992 | Val Acc: 0.4970\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [01:00<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7877 | Train Acc: 0.6156\n",
            "Val Loss: 0.7886 | Val Acc: 0.6369\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6258 | Train Acc: 0.7382\n",
            "Val Loss: 0.6218 | Val Acc: 0.7411\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [01:00<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4291 | Train Acc: 0.8379\n",
            "Val Loss: 0.4642 | Val Acc: 0.8333\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3749 | Train Acc: 0.8624\n",
            "Val Loss: 0.6760 | Val Acc: 0.7202\n",
            "\n",
            "Epoch 6/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2769 | Train Acc: 0.9008\n",
            "Val Loss: 0.4559 | Val Acc: 0.8185\n",
            "\n",
            "Epoch 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3393 | Train Acc: 0.8836\n",
            "Val Loss: 0.3241 | Val Acc: 0.8869\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [01:00<00:00,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1765 | Train Acc: 0.9370\n",
            "Val Loss: 0.5422 | Val Acc: 0.8095\n",
            "\n",
            "Epoch 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1993 | Train Acc: 0.9359\n",
            "Val Loss: 0.3224 | Val Acc: 0.8929\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [01:00<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1518 | Train Acc: 0.9482\n",
            "Val Loss: 0.9116 | Val Acc: 0.7530\n",
            "\n",
            "Epoch 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1879 | Train Acc: 0.9365\n",
            "Val Loss: 1.7777 | Val Acc: 0.6905\n",
            "\n",
            "Epoch 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4305 | Train Acc: 0.8507\n",
            "Val Loss: 0.3312 | Val Acc: 0.8869\n",
            "\n",
            "Epoch 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1965 | Train Acc: 0.9326\n",
            "Val Loss: 0.4267 | Val Acc: 0.8244\n",
            "\n",
            "Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1508 | Train Acc: 0.9476\n",
            "Val Loss: 0.5957 | Val Acc: 0.8155\n",
            "\n",
            "Epoch 15/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:59<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1245 | Train Acc: 0.9543\n",
            "Val Loss: 0.5556 | Val Acc: 0.8423\n",
            "\n",
            "ğŸ§ª Testing best model on test set...\n",
            "ğŸƒ View run clean-turtle-535 at: https://dagshub.com/Zappy17/research_paper.mlflow/#/experiments/0/runs/d9af4b4701eb4efbb969f996612cd175\n",
            "ğŸ§ª View experiment at: https://dagshub.com/Zappy17/research_paper.mlflow/#/experiments/0\n",
            "\n",
            "âœ… Training complete. Best model and metrics logged to MLflow.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Paths\n",
        "train_dir = '/content/split_data/train'\n",
        "val_dir = '/content/split_data/val'\n",
        "test_dir = '/content/split_data/test'\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "num_classes = 3\n",
        "lr = 3e-4\n",
        "num_epochs = 15\n",
        "image_size = 224\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# Load model\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training function\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "    for images, labels in tqdm(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# Evaluation function\n",
        "def eval_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# MLflow logging\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    mlflow.log_params({\n",
        "        \"model\": \"vit_base_patch16_224\",\n",
        "        \"image_size\": image_size,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": lr,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"num_classes\": num_classes,\n",
        "    })\n",
        "\n",
        "    best_val_acc = 0\n",
        "    model_path = \"best_vit_lung_cancer_model.pth\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Save best model and metrics\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_metrics = {\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_acc\": train_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_acc\": val_acc\n",
        "            }\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(\"âœ… Saved Best Model\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    model_size_mb = os.path.getsize(model_path) / (1024 ** 2)\n",
        "\n",
        "    # Load best model and test\n",
        "    print(\"\\nğŸ§ª Testing best model on test set...\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    test_loss, test_acc = eval_epoch(model, test_loader, criterion)\n",
        "\n",
        "    # Final logging\n",
        "    mlflow.log_metrics({\n",
        "        **best_metrics,\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"training_time_sec\": total_time,\n",
        "        \"model_size_mb\": model_size_mb\n",
        "    })\n",
        "\n",
        "    mlflow.log_artifact(model_path)\n",
        "\n",
        "\n",
        "print(\"\\nâœ… Training complete. Best model and metrics logged to MLflow.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEcadsmwrhpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608bcca8-8d40-4d33-849f-b2813996c5af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Classes: ['Bengin cases', 'Malignant cases', 'Normal cases']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528M/528M [00:05<00:00, 97.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3092 | Train Acc: 0.8975\n",
            "Val Loss: 0.2108 | Val Acc: 0.9315\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1446 | Train Acc: 0.9510\n",
            "Val Loss: 0.1980 | Val Acc: 0.9256\n",
            "\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1179 | Train Acc: 0.9588\n",
            "Val Loss: 0.1803 | Val Acc: 0.9435\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0971 | Train Acc: 0.9733\n",
            "Val Loss: 0.1073 | Val Acc: 0.9554\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0718 | Train Acc: 0.9744\n",
            "Val Loss: 0.0918 | Val Acc: 0.9762\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 6/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0740 | Train Acc: 0.9788\n",
            "Val Loss: 0.2268 | Val Acc: 0.9405\n",
            "\n",
            "Epoch 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1950 | Train Acc: 0.9560\n",
            "Val Loss: 0.4086 | Val Acc: 0.9464\n",
            "\n",
            "Epoch 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3185 | Train Acc: 0.9565\n",
            "Val Loss: 0.2626 | Val Acc: 0.9732\n",
            "\n",
            "Epoch 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1372 | Train Acc: 0.9805\n",
            "Val Loss: 0.3138 | Val Acc: 0.9762\n",
            "\n",
            "Epoch 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0598 | Train Acc: 0.9872\n",
            "Val Loss: 0.5050 | Val Acc: 0.9286\n",
            "\n",
            "Epoch 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0479 | Train Acc: 0.9855\n",
            "Val Loss: 0.2991 | Val Acc: 0.9762\n",
            "\n",
            "Epoch 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0615 | Train Acc: 0.9877\n",
            "Val Loss: 0.2386 | Val Acc: 0.9702\n",
            "\n",
            "Epoch 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0602 | Train Acc: 0.9861\n",
            "Val Loss: 0.2470 | Val Acc: 0.9762\n",
            "\n",
            "Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0487 | Train Acc: 0.9928\n",
            "Val Loss: 0.4076 | Val Acc: 0.9613\n",
            "\n",
            "Epoch 15/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0498 | Train Acc: 0.9894\n",
            "Val Loss: 0.2919 | Val Acc: 0.9583\n",
            "\n",
            "ğŸ§ª Test Loss: 0.1219 | Test Accuracy: 0.9714\n",
            "â±ï¸ Training Time: 254.54s | ğŸ’¾ Model Size: 512.22MB\n",
            "ğŸƒ View run nervous-sponge-591 at: https://dagshub.com/Zappy17/research_paper.mlflow/#/experiments/0/runs/ecf5f72d69904f42897082b7a1b64ef9\n",
            "ğŸ§ª View experiment at: https://dagshub.com/Zappy17/research_paper.mlflow/#/experiments/0\n",
            "\n",
            "âœ… Training complete. Best model and metrics logged to MLflow.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Paths\n",
        "train_dir = '/content/split_data/train'\n",
        "val_dir = '/content/split_data/val'\n",
        "test_dir = '/content/split_data/test'\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "num_classes = 3\n",
        "lr = 3e-4\n",
        "num_epochs = 15\n",
        "image_size = 224\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# Load and modify VGG16\n",
        "model = models.vgg16(pretrained=True)\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False  # Freeze conv base\n",
        "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def eval_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# Start MLflow logging\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params({\n",
        "        \"model\": \"vgg16\",\n",
        "        \"image_size\": image_size,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": lr,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"frozen_base\": True\n",
        "    })\n",
        "\n",
        "    best_val_acc = 0\n",
        "    model_path = \"best_vgg16_lung_model.pth\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            best_metrics = {\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_acc\": train_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_acc\": val_acc\n",
        "            }\n",
        "            print(\"âœ… Saved Best Model\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    model_size_mb = os.path.getsize(model_path) / (1024 ** 2)\n",
        "\n",
        "    # Load best model for testing\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    test_loss, test_acc = eval_epoch(model, test_loader, criterion)\n",
        "\n",
        "    # Log final best metrics\n",
        "    mlflow.log_metrics({\n",
        "        **best_metrics,\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"training_time_sec\": total_time,\n",
        "        \"model_size_mb\": model_size_mb\n",
        "    })\n",
        "\n",
        "    mlflow.log_artifact(model_path)\n",
        "\n",
        "    print(f\"\\nğŸ§ª Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"â±ï¸ Training Time: {total_time:.2f}s | ğŸ’¾ Model Size: {model_size_mb:.2f}MB\")\n",
        "\n",
        "print(\"\\nâœ… Training complete. Best model and metrics logged to MLflow.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import mlflow\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# âœ… Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# âœ… Paths\n",
        "train_dir = '/content/split_data/train'\n",
        "val_dir = '/content/split_data/val'\n",
        "test_dir = '/content/split_data/test'\n",
        "\n",
        "# âœ… Hyperparameters\n",
        "batch_size = 32\n",
        "num_classes = 3\n",
        "lr = 3e-4\n",
        "num_epochs = 15\n",
        "image_size = 224\n",
        "\n",
        "# âœ… Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# âœ… Datasets & loaders\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# âœ… Load ViT model\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=num_classes)\n",
        "\n",
        "# â„ï¸ Freeze first 10 transformer blocks\n",
        "for name, param in model.named_parameters():\n",
        "    if any(name.startswith(f'blocks.{i}') for i in range(10)):\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# âœ… Loss & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "# âœ… Train loop\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "    for images, labels in tqdm(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# âœ… Eval loop\n",
        "def eval_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# âœ… MLflow logging for best model\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params({\n",
        "        \"model\": \"vit_base_patch16_224\",\n",
        "        \"fine_tuned_blocks\": \"blocks.10, blocks.11 + head\",\n",
        "        \"image_size\": image_size,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": lr,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"num_classes\": num_classes,\n",
        "    })\n",
        "\n",
        "    best_val_acc = 0\n",
        "    model_path = \"best_vit_frozen_model.pth\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(\"âœ… Saved Best Model\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nâ±ï¸ Total training time: {total_time:.2f} seconds\")\n",
        "    mlflow.log_metric(\"training_time_sec\", total_time)\n",
        "\n",
        "    # âœ… Model size\n",
        "    model_size_mb = os.path.getsize(model_path) / (1024 ** 2)\n",
        "    print(f\"ğŸ’¾ Model size: {model_size_mb:.2f} MB\")\n",
        "    mlflow.log_metric(\"model_size_mb\", model_size_mb)\n",
        "\n",
        "    # âœ… Test best model\n",
        "    print(\"\\nğŸ§ª Testing best model on test set...\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    test_loss, test_acc = eval_epoch(model, test_loader, criterion)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    mlflow.log_metrics({\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"best_val_acc\": best_val_acc\n",
        "    })\n",
        "\n",
        "    # âœ… Log best model\n",
        "    try:\n",
        "        mlflow.pytorch.log_model(model, artifact_path=\"vit_finetuned_best\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Warning: Failed to log model: {e}\")\n",
        "\n",
        "print(\"\\nâœ… Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY-paCsjXMX8",
        "outputId": "53e660e4-9972-4967-b5f4-231483e9d9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Classes: ['Bengin cases', 'Malignant cases', 'Normal cases']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /Zappy17/research_paper.mlflow/api/2.0/mlflow/runs/create\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:44<00:00,  1.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5633 | Train Acc: 0.7850\n",
            "Val Loss: 0.4000 | Val Acc: 0.8631\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2670 | Train Acc: 0.9031\n",
            "Val Loss: 0.3169 | Val Acc: 0.8690\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1293 | Train Acc: 0.9593\n",
            "Val Loss: 0.1696 | Val Acc: 0.9554\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1000 | Train Acc: 0.9688\n",
            "Val Loss: 0.1271 | Val Acc: 0.9702\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0426 | Train Acc: 0.9894\n",
            "Val Loss: 0.0877 | Val Acc: 0.9732\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 6/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0389 | Train Acc: 0.9889\n",
            "Val Loss: 0.1359 | Val Acc: 0.9524\n",
            "\n",
            "Epoch 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0353 | Train Acc: 0.9933\n",
            "Val Loss: 0.0896 | Val Acc: 0.9673\n",
            "\n",
            "Epoch 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0303 | Train Acc: 0.9928\n",
            "Val Loss: 0.0696 | Val Acc: 0.9881\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0157 | Train Acc: 0.9950\n",
            "Val Loss: 0.0644 | Val Acc: 0.9762\n",
            "\n",
            "Epoch 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0137 | Train Acc: 0.9972\n",
            "Val Loss: 0.0489 | Val Acc: 0.9911\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0082 | Train Acc: 0.9983\n",
            "Val Loss: 0.0648 | Val Acc: 0.9792\n",
            "\n",
            "Epoch 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0078 | Train Acc: 0.9989\n",
            "Val Loss: 0.0706 | Val Acc: 0.9732\n",
            "\n",
            "Epoch 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0041 | Train Acc: 1.0000\n",
            "Val Loss: 0.0635 | Val Acc: 0.9851\n",
            "\n",
            "Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0041 | Train Acc: 0.9994\n",
            "Val Loss: 0.0661 | Val Acc: 0.9851\n",
            "\n",
            "Epoch 15/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:43<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0012 | Train Acc: 1.0000\n",
            "Val Loss: 0.0611 | Val Acc: 0.9851\n",
            "\n",
            "â±ï¸ Total training time: 727.09 seconds\n",
            "ğŸ’¾ Model size: 327.36 MB\n",
            "\n",
            "ğŸ§ª Testing best model on test set...\n",
            "Test Loss: 0.0694 | Test Accuracy: 0.9762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/06/12 23:09:58 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Warning: Failed to log model: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n",
            "ğŸƒ View run whimsical-penguin-55 at: https://dagshub.com/Zappy17/research_paper.mlflow/#/experiments/0/runs/01e934c7a63e4daabc83e86718114985\n",
            "ğŸ§ª View experiment at: https://dagshub.com/Zappy17/research_paper.mlflow/#/experiments/0\n",
            "\n",
            "âœ… Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import mlflow\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# âœ… Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# âœ… Dataset paths\n",
        "train_dir = '/content/split_data/train'\n",
        "val_dir = '/content/split_data/val'\n",
        "test_dir = '/content/split_data/test'\n",
        "\n",
        "# âœ… Hyperparameters\n",
        "batch_size = 32\n",
        "num_classes = 3\n",
        "lr = 3e-4\n",
        "num_epochs = 15\n",
        "image_size = 224\n",
        "\n",
        "# âœ… Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# âœ… Load datasets\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# âœ… Load pretrained VGG16 model\n",
        "model = timm.create_model('vgg16', pretrained=True, num_classes=num_classes)\n",
        "\n",
        "# â„ï¸ Freeze all feature extractor layers\n",
        "for name, param in model.named_parameters():\n",
        "    if \"features\" in name:\n",
        "        param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# âœ… Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "# âœ… Train one epoch\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "    for images, labels in tqdm(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# âœ… Evaluation\n",
        "def eval_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# âœ… Start MLflow run\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params({\n",
        "        \"model\": \"vgg16\",\n",
        "        \"frozen_layers\": \"all feature extractor layers\",\n",
        "        \"image_size\": image_size,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": lr,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"num_classes\": num_classes,\n",
        "    })\n",
        "\n",
        "    best_val_acc = 0\n",
        "    model_path = \"best_vgg16_model.pth\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(\"âœ… Saved Best Model\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nâ±ï¸ Total training time: {total_time:.2f} seconds\")\n",
        "    mlflow.log_metric(\"training_time_sec\", total_time)\n",
        "\n",
        "    # âœ… Model size\n",
        "    model_size_mb = os.path.getsize(model_path) / (1024 ** 2)\n",
        "    print(f\"ğŸ’¾ Model size: {model_size_mb:.2f} MB\")\n",
        "    mlflow.log_metric(\"model_size_mb\", model_size_mb)\n",
        "\n",
        "    # âœ… Evaluate on test set\n",
        "    print(\"\\nğŸ§ª Testing best model on test set...\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    test_loss, test_acc = eval_epoch(model, test_loader, criterion)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    mlflow.log_metrics({\n",
        "        \"test_loss\": test_loss,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"best_val_acc\": best_val_acc\n",
        "    })\n",
        "\n",
        "    # âœ… Try logging model\n",
        "    try:\n",
        "        mlflow.pytorch.log_model(model, artifact_path=\"vgg16_finetuned_best\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Warning: Could not log model: {e}\")\n",
        "\n",
        "print(\"\\nâœ… VGG16 training complete and best model logged.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1f626a64e3e64cfa9c2e75c83f2b6d68",
            "990871f3f80b42a08fa96635172158e1",
            "3bb740cde5cb4c6681a824ca2539c308",
            "408e09e3ccde43fe9342aac067c71415",
            "d22ba4a391d74234a31da79724cc8873",
            "2897d556d22e46c086369b92100dcc3c",
            "4919e78939544cfeb5969d48e482afcf",
            "b8ecc605ab0d48de967853ded3baab76",
            "ed9e5c1f027c41968d93be45238691f8",
            "9f6efa5e24af4c468a0f9aeec0bee5c3",
            "bfd6a2e448df44a7848517a468b82df0"
          ]
        },
        "id": "5NYnX8Djb-km",
        "outputId": "c9f18f18-f8da-45cd-87a2-952e609fd7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Classes: ['Bengin cases', 'Malignant cases', 'Normal cases']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f626a64e3e64cfa9c2e75c83f2b6d68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /Zappy17/research_paper.mlflow/api/2.0/mlflow/runs/create\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  3.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2538 | Train Acc: 0.9103\n",
            "Val Loss: 0.2566 | Val Acc: 0.9077\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0734 | Train Acc: 0.9705\n",
            "Val Loss: 0.2186 | Val Acc: 0.9494\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0327 | Train Acc: 0.9877\n",
            "Val Loss: 0.1767 | Val Acc: 0.9554\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0600 | Train Acc: 0.9822\n",
            "Val Loss: 0.1543 | Val Acc: 0.9613\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0142 | Train Acc: 0.9955\n",
            "Val Loss: 0.1793 | Val Acc: 0.9702\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 6/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0125 | Train Acc: 0.9955\n",
            "Val Loss: 0.2112 | Val Acc: 0.9554\n",
            "\n",
            "Epoch 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0232 | Train Acc: 0.9911\n",
            "Val Loss: 0.1834 | Val Acc: 0.9524\n",
            "\n",
            "Epoch 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0567 | Train Acc: 0.9838\n",
            "Val Loss: 0.5910 | Val Acc: 0.9107\n",
            "\n",
            "Epoch 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0461 | Train Acc: 0.9844\n",
            "Val Loss: 0.3362 | Val Acc: 0.9613\n",
            "\n",
            "Epoch 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:13<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0332 | Train Acc: 0.9894\n",
            "Val Loss: 0.1865 | Val Acc: 0.9702\n",
            "\n",
            "Epoch 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  3.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0059 | Train Acc: 0.9983\n",
            "Val Loss: 0.3186 | Val Acc: 0.9554\n",
            "\n",
            "Epoch 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0157 | Train Acc: 0.9944\n",
            "Val Loss: 0.1950 | Val Acc: 0.9643\n",
            "\n",
            "Epoch 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0065 | Train Acc: 0.9972\n",
            "Val Loss: 0.2364 | Val Acc: 0.9732\n",
            "âœ… Saved Best Model\n",
            "\n",
            "Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0028 | Train Acc: 0.9989\n",
            "Val Loss: 0.3071 | Val Acc: 0.9583\n",
            "\n",
            "Epoch 15/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57/57 [00:14<00:00,  4.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0051 | Train Acc: 0.9983\n",
            "Val Loss: 0.2162 | Val Acc: 0.9792\n",
            "âœ… Saved Best Model\n",
            "\n",
            "â±ï¸ Total training time: 289.09 seconds\n",
            "ğŸ’¾ Model size: 512.22 MB\n",
            "\n",
            "ğŸ§ª Testing best model on test set...\n",
            "Test Loss: 0.0979 | Test Accuracy: 0.9810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/06/12 23:28:51 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Warning: Could not log model: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n",
            "ğŸƒ View run luminous-perch-562 at: https://dagshub.com/Zappy17/research_paper.mlflow/#/experiments/0/runs/e79fb608341045f394a184d736194f50\n",
            "ğŸ§ª View experiment at: https://dagshub.com/Zappy17/research_paper.mlflow/#/experiments/0\n",
            "\n",
            "âœ… VGG16 training complete and best model logged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import mlflow\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Paths\n",
        "train_dir = '/content/split_data/train'\n",
        "val_dir = '/content/split_data/val'\n",
        "test_dir = '/content/split_data/test'\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "num_classes = 3\n",
        "lr = 3e-4\n",
        "num_epochs = 15\n",
        "image_size = 224\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Datasets & Loaders\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# Load COVID ViT pretrained model\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"DunnBC22/vit-base-patch16-224-in21k_covid_19_ct_scans\"\n",
        ")\n",
        "\n",
        "# Modify classifier head\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(model.classifier.in_features, num_classes)\n",
        ")\n",
        "\n",
        "# Freeze all ViT encoder layers\n",
        "for name, param in model.named_parameters():\n",
        "    if not name.startswith(\"classifier\"):\n",
        "        param.requires_grad = False\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "# Training & Evaluation loops\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def eval_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# MLflow logging\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params({\n",
        "        \"model\": \"covid_vit_transfer_learning\",\n",
        "        \"image_size\": image_size,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": lr,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"num_classes\": num_classes\n",
        "    })\n",
        "\n",
        "    best_val_acc = 0\n",
        "    model_path = \"covid_vit_lung_teacher.pth\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(\"âœ… Saved Best Model\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    mlflow.log_metric(\"training_time_sec\", total_time)\n",
        "    mlflow.log_metric(\"val_best_accuracy\", best_val_acc)\n",
        "\n",
        "    # Log model size\n",
        "    model_size = os.path.getsize(model_path) / (1024 ** 2)\n",
        "    mlflow.log_metric(\"model_size_mb\", model_size)\n",
        "\n",
        "    # Test evaluation\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    test_loss, test_acc = eval_epoch(model, test_loader, criterion)\n",
        "    mlflow.log_metrics({\"test_loss\": test_loss, \"test_accuracy\": test_acc})\n",
        "\n",
        "    # Log model\n",
        "    try:\n",
        "        mlflow.pytorch.log_model(model, \"covid_vit_teacher\")\n",
        "    except:\n",
        "        print(\"âš ï¸ Could not log model to MLflow (likely due to custom transformers model)\")\n",
        "\n",
        "print(\"\\nâœ… Training complete. Teacher ViT model ready.\")\n"
      ],
      "metadata": {
        "id": "dIiITWi4w0VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Paths\n",
        "train_dir = '/content/split_data/train'\n",
        "val_dir = '/content/split_data/val'\n",
        "test_dir = '/content/split_data/test'\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "num_classes = 3\n",
        "lr = 3e-4\n",
        "num_epochs = 15\n",
        "image_size = 224\n",
        "\n",
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Datasets & Loaders\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Classes: {train_dataset.classes}\")\n",
        "\n",
        "# Load COVID ViT pretrained model\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"DunnBC22/vit-base-patch16-224-in21k_covid_19_ct_scans\"\n",
        ")\n",
        "\n",
        "# Modify classifier head\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(model.classifier.in_features, num_classes)\n",
        ")\n",
        "\n",
        "# Freeze all ViT encoder layers\n",
        "for name, param in model.named_parameters():\n",
        "    if not name.startswith(\"classifier\"):\n",
        "        param.requires_grad = False\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "\n",
        "# Training & Evaluation loops\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in tqdm(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "def eval_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "# MLflow logging\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params({\n",
        "        \"model\": \"covid_vit_transfer_learning\",\n",
        "        \"image_size\": image_size,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": lr,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"num_classes\": num_classes\n",
        "    })\n",
        "\n",
        "    best_val_acc = 0\n",
        "    model_path = \"covid_vit_lung_teacher.pth\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "        val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(\"âœ… Saved Best Model\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    mlflow.log_metric(\"training_time_sec\", total_time)\n",
        "    mlflow.log_metric(\"val_best_accuracy\", best_val_acc)\n",
        "\n",
        "    # Log model size\n",
        "    model_size = os.path.getsize(model_path) / (1024 ** 2)\n",
        "    mlflow.log_metric(\"model_size_mb\", model_size)\n",
        "\n",
        "    # Test evaluation\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    test_loss, test_acc = eval_epoch(model, test_loader, criterion)\n",
        "    mlflow.log_metrics({\"test_loss\": test_loss, \"test_accuracy\": test_acc})\n",
        "\n",
        "    # Log model\n",
        "    try:\n",
        "        mlflow.pytorch.log_model(model, \"covid_vit_teacher\")\n",
        "    except:\n",
        "        print(\"âš ï¸ Could not log model to MLflow (likely due to custom transformers model)\")\n",
        "\n",
        "print(\"\\nâœ… Training complete. Teacher ViT model ready.\")\n"
      ],
      "metadata": {
        "id": "Xuwk4t_1xGk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assumes the same data, transforms, loaders from the teacher code\n",
        "# Uses: train_loader, val_loader, test_loader, num_classes, device\n",
        "\n",
        "# ---------------------\n",
        "# Define Distillation Loss\n",
        "# ---------------------\n",
        "def distillation_loss(student_outputs, teacher_outputs, labels, T=4, alpha=0.7):\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Soften probabilities\n",
        "    student_probs = nn.functional.log_softmax(student_outputs / T, dim=1)\n",
        "    teacher_probs = nn.functional.softmax(teacher_outputs / T, dim=1)\n",
        "\n",
        "    distill = kl_loss(student_probs, teacher_probs) * (T * T)\n",
        "    student_loss = ce_loss(student_outputs, labels)\n",
        "\n",
        "    return alpha * student_loss + (1 - alpha) * distill\n",
        "\n",
        "# ---------------------\n",
        "# Load trained teacher\n",
        "# ---------------------\n",
        "from transformers import AutoModelForImageClassification\n",
        "teacher_model = AutoModelForImageClassification.from_pretrained(\n",
        "    \"DunnBC22/vit-base-patch16-224-in21k_covid_19_ct_scans\"\n",
        ")\n",
        "teacher_model.classifier = nn.Sequential(nn.Linear(768, num_classes))\n",
        "teacher_model.load_state_dict(torch.load(\"covid_vit_lung_teacher.pth\"))\n",
        "teacher_model.to(device)\n",
        "teacher_model.eval()\n",
        "\n",
        "# ---------------------\n",
        "# Load student (VGG16)\n",
        "# ---------------------\n",
        "student_model = models.vgg16(pretrained=True)\n",
        "for param in student_model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "student_model.classifier[6] = nn.Linear(4096, num_classes)\n",
        "student_model.to(device)\n",
        "\n",
        "# ---------------------\n",
        "# Training loop\n",
        "# ---------------------\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=3e-4)\n",
        "num_epochs = 15\n",
        "\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_params({\n",
        "        \"student_model\": \"VGG16\",\n",
        "        \"teacher_model\": \"COVID-ViT\",\n",
        "        \"distill_T\": 4,\n",
        "        \"alpha\": 0.7,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"num_epochs\": num_epochs,\n",
        "    })\n",
        "\n",
        "    best_val_acc = 0\n",
        "    model_path = \"distilled_vgg16.pth\"\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        student_model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            student_outputs = student_model(images)\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(images).logits\n",
        "\n",
        "            loss = distillation_loss(student_outputs, teacher_outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(student_outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        train_loss = total_loss / total\n",
        "\n",
        "        student_model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = student_model(images)\n",
        "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(student_model.state_dict(), model_path)\n",
        "            print(\"âœ… Saved best student model\")\n",
        "\n",
        "    # Final test\n",
        "    student_model.load_state_dict(torch.load(model_path))\n",
        "    student_model.eval()\n",
        "    test_loss, test_correct, test_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = student_model(images)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_correct += (preds == labels).sum().item()\n",
        "            test_total += labels.size(0)\n",
        "\n",
        "    test_acc = test_correct / test_total\n",
        "    mlflow.log_metrics({\n",
        "        \"val_best_accuracy\": best_val_acc,\n",
        "        \"test_accuracy\": test_acc,\n",
        "        \"test_loss\": test_loss / test_total\n",
        "    })\n",
        "\n",
        "    mlflow.log_artifact(model_path)\n",
        "\n",
        "print(\"\\nâœ… Knowledge Distillation complete. Student model trained.\")\n"
      ],
      "metadata": {
        "id": "-QzkoSodM7KB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f626a64e3e64cfa9c2e75c83f2b6d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_990871f3f80b42a08fa96635172158e1",
              "IPY_MODEL_3bb740cde5cb4c6681a824ca2539c308",
              "IPY_MODEL_408e09e3ccde43fe9342aac067c71415"
            ],
            "layout": "IPY_MODEL_d22ba4a391d74234a31da79724cc8873"
          }
        },
        "990871f3f80b42a08fa96635172158e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2897d556d22e46c086369b92100dcc3c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4919e78939544cfeb5969d48e482afcf",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "3bb740cde5cb4c6681a824ca2539c308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8ecc605ab0d48de967853ded3baab76",
            "max": 553432986,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed9e5c1f027c41968d93be45238691f8",
            "value": 553432986
          }
        },
        "408e09e3ccde43fe9342aac067c71415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f6efa5e24af4c468a0f9aeec0bee5c3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bfd6a2e448df44a7848517a468b82df0",
            "value": "â€‡553M/553Mâ€‡[00:06&lt;00:00,â€‡179MB/s]"
          }
        },
        "d22ba4a391d74234a31da79724cc8873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2897d556d22e46c086369b92100dcc3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4919e78939544cfeb5969d48e482afcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8ecc605ab0d48de967853ded3baab76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9e5c1f027c41968d93be45238691f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f6efa5e24af4c468a0f9aeec0bee5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfd6a2e448df44a7848517a468b82df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}